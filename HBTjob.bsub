#!/bin/bash
# this file is intended to be used with bsub, for job submition
##BSUB -L /bin/bash       # script shell language (/bin/tcsh, /bin/ksh etc.) NOTE: this cause problem on SHAO cluster of missing libimf.so
#BSUB -n 32               # number of cores required
#BSUB -M 100000        #max (MB)
##BSUB -m "cosma-f"
##BSUB -R "rusage[mem=3000]"  #reserve 3gb
#BSUB -J HBT               # name of job
#BSUB -o HBT.Log.%J    # log file for standard output
#BSUB -e HBT.Log.%J     # log file for standard error
##BSUB -q normal              # target queue for job execution
#BSUB -q shm4
#BSUB -P durham             #project to charge time; durham or dp004
##BSUB -u hanjiaxin@gmail.com
##BSUB -N
##BSUB -a intelmpi
#BSUB -x                     # give node exclusive access to job
#BSUB -R "span[ptile=32]"     # number of processors to use per node
##BSUB -R "model=SandyBridge"


ulimit -s unlimited

#~ source /etc/profile.d/modules.csh

export HBT_VERSION=8.7c
export OMP_SCHEDULE=dynamic
export OMP_DYNAMIC=FALSE # this works for icc v11.1.072, 
# but seems not always use the maximum available number of cpus allowed by NUM_THREADS
# so better not allow it to adjust. shit.
export KMP_BLOCKTIME=0  # this seems to help a lot, to reuse thread immediately after its job is finished
export OMP_NUM_THREADS=32
export OMP_NESTED=0
export RUN_NUM=AqB2


date 
echo "BT.$RUN_NUM started using schedule $OMP_SCHEDULE and omp_dynamic state is $OMP_DYNAMIC" 
echo "with $OMP_NUM_THREADS threads" 

snapstart=1
snapend=127
./HBT.$RUN_NUM $snapstart $snapend

./haloprof.$RUN_NUM $snapend
./snaps.$RUN_NUM $snapend 0 $snapend
./snap2.$RUN_NUM $snapend 0 $snapend
./massfun_plot.$RUN_NUM $snapend 0

for((i=$(($snapend-1));i>=$snapstart;i-=1))
do
   echo "Snap=$i:" 
   ./haloprof.$RUN_NUM $i
done
chmod a-w ~/data/$RUN_NUM/subcat/profile/logbin/*

./NFW_fit.$RUN_NUM
